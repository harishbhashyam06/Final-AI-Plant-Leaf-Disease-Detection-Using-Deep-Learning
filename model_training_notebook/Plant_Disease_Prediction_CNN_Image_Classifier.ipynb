{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8VSns6fO6Pg"
      },
      "source": [
        "**Seeding for reproducibility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JSu8kpnEHDPB"
      },
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyqk5_8AO1Kr"
      },
      "source": [
        "**Importing the dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "16dILovOOFy0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gAnTOlEPR8a"
      },
      "source": [
        "**Data Curation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT4tQUqBs90l"
      },
      "source": [
        "Upload the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKWvyGVDtALx",
        "outputId": "f565ec66-6b79-4d7f-ce6f-ecb12f388e37"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZM5gnAAVtH0s"
      },
      "outputs": [],
      "source": [
        "kaggle_credentails = json.load(open(\"kaggle.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xWS6H5mPtNa_"
      },
      "outputs": [],
      "source": [
        "# setup Kaggle API key as environment variables\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_credentails[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_credentails[\"key\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypPVDLobtUr5",
        "outputId": "53a70f81-a8ed-4287-f1f5-678b465142d0"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d abdallahalidev/plantvillage-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20t7J2zctdou",
        "outputId": "71e8ea49-eac0-4f1f-b13c-f59595733d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kaggle.json  plantvillage-dataset.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cjVbT1ZItYe2"
      },
      "outputs": [],
      "source": [
        "# Unzip the downloaded dataset\n",
        "with ZipFile(\"plantvillage-dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_5Oa9WPtfXr",
        "outputId": "79a1b2c7-ca9c-4a89-febe-17abc7f399d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['grayscale', 'segmented', 'color']\n",
            "38\n",
            "['Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Bacterial_spot', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___healthy']\n",
            "38\n",
            "['Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Bacterial_spot', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___healthy']\n",
            "38\n",
            "['Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Bacterial_spot', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___healthy']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"plantvillage dataset\"))\n",
        "\n",
        "\n",
        "print(len(os.listdir(\"plantvillage dataset/segmented\")))\n",
        "print(os.listdir(\"plantvillage dataset/segmented\")[:5])\n",
        "\n",
        "print(len(os.listdir(\"plantvillage dataset/color\")))\n",
        "print(os.listdir(\"plantvillage dataset/color\")[:5])\n",
        "\n",
        "print(len(os.listdir(\"plantvillage dataset/grayscale\")))\n",
        "print(os.listdir(\"plantvillage dataset/grayscale\")[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snyC_-2jt0z3"
      },
      "source": [
        "**Number of Classes = 38**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFR52Pk6tp2U",
        "outputId": "4917ce76-17f2-4103-85ca-14d2af84dc06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "423\n",
            "['05cff9d7-0f63-4b6e-9aa3-199cf9ffa64c___Mt.N.V_HL 9111.JPG', '0ce12a10-c6ff-494e-a927-5ddc809c707a___Mt.N.V_HL 8945.JPG', 'c839e134-6692-4065-8e12-7ea01adcc794___Mt.N.V_HL 9014.JPG', '452823c1-22d2-4bce-a4cc-8ad014421261___Mt.N.V_HL 6199.JPG', 'c48a4277-3bd4-45c8-be06-8bdc4404f252___Mt.N.V_HL 6137.JPG']\n"
          ]
        }
      ],
      "source": [
        "print(len(os.listdir(\"plantvillage dataset/color/Grape___healthy\")))\n",
        "print(os.listdir(\"plantvillage dataset/color/Grape___healthy\")[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhEi6mbpt4aD"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WlqvsdtBttrh"
      },
      "outputs": [],
      "source": [
        "# Dataset Path\n",
        "base_dir = 'plantvillage dataset/color'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XQxHLLbKt6oF",
        "outputId": "960d3327-8801-4a10-e6c5-f5551337a781"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/plantvillage dataset/color/Apple___Cedar_apple_rust/025b2b9a-0ec4-4132-96ac-7f2832d0db4a___FREC_C.Rust 3655.JPG'\n",
        "\n",
        "# Read the image\n",
        "img = mpimg.imread(image_path)\n",
        "\n",
        "print(img.shape)\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Turn off axis numbers\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVgo1I9sDP6B",
        "outputId": "302d2d4b-6f7c-4ddf-ab9e-a9d6bd738b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[179 175 176]\n",
            "  [181 177 178]\n",
            "  [184 180 181]\n",
            "  ...\n",
            "  [115 112 105]\n",
            "  [108 105  98]\n",
            "  [101  98  91]]\n",
            "\n",
            " [[176 172 173]\n",
            "  [177 173 174]\n",
            "  [178 174 175]\n",
            "  ...\n",
            "  [113 110 103]\n",
            "  [111 108 101]\n",
            "  [109 106  99]]\n",
            "\n",
            " [[180 176 177]\n",
            "  [180 176 177]\n",
            "  [180 176 177]\n",
            "  ...\n",
            "  [108 105  98]\n",
            "  [111 108 101]\n",
            "  [114 111 104]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[137 128 119]\n",
            "  [131 122 113]\n",
            "  [125 116 107]\n",
            "  ...\n",
            "  [ 74  65  48]\n",
            "  [ 74  65  48]\n",
            "  [ 73  64  47]]\n",
            "\n",
            " [[136 127 118]\n",
            "  [132 123 114]\n",
            "  [128 119 110]\n",
            "  ...\n",
            "  [ 77  69  50]\n",
            "  [ 75  67  48]\n",
            "  [ 75  67  48]]\n",
            "\n",
            " [[133 124 115]\n",
            "  [133 124 115]\n",
            "  [132 123 114]\n",
            "  ...\n",
            "  [ 81  73  54]\n",
            "  [ 80  72  53]\n",
            "  [ 79  71  52]]]\n"
          ]
        }
      ],
      "source": [
        "image_path = '/content/plantvillage dataset/color/Apple___Cedar_apple_rust/025b2b9a-0ec4-4132-96ac-7f2832d0db4a___FREC_C.Rust 3655.JPG'\n",
        "\n",
        "# Read the image\n",
        "img = mpimg.imread(image_path)\n",
        "\n",
        "print(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "w6S1jYo0u5o-"
      },
      "outputs": [],
      "source": [
        "# Image Parameters\n",
        "img_size = 224\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcovy3vxvf31"
      },
      "source": [
        "**Train Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zoJjajTcvTae"
      },
      "outputs": [],
      "source": [
        "# Image Data Generators\n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # Use 20% of data for validation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnwsA5IPvWNG",
        "outputId": "be155ed6-aa77-4f28-c202-94fba6ea59bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 43456 images belonging to 38 classes.\n"
          ]
        }
      ],
      "source": [
        "# Train Generator\n",
        "train_generator = data_gen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    subset='training',\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtxNLvmbvYNX",
        "outputId": "a60d0b2a-bba4-4595-d5e1-e9f6978ddbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10849 images belonging to 38 classes.\n"
          ]
        }
      ],
      "source": [
        "# Validation Generator\n",
        "validation_generator = data_gen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(img_size, img_size),\n",
        "    batch_size=batch_size,\n",
        "    subset='validation',\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE4vUKMkviT8"
      },
      "source": [
        "**Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VUsvwveevZ-m"
      },
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(train_generator.num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9qJo-GSvoIB",
        "outputId": "56ed059c-85f2-4490-8dfc-63e25516d2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 186624)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               47776000  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 38)                9766      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 47805158 (182.36 MB)\n",
            "Trainable params: 47805158 (182.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PKi-ot0xvpC8"
      },
      "outputs": [],
      "source": [
        "# Compile the Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCBezWyLv1hF"
      },
      "source": [
        "**Model training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSvHhJqevyjE",
        "outputId": "51052757-e403-4ed0-87b0-42ff1ff6451b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1358/1358 [==============================] - 108s 76ms/step - loss: 0.9791 - accuracy: 0.7328 - val_loss: 0.4846 - val_accuracy: 0.8465\n",
            "Epoch 2/5\n",
            "1358/1358 [==============================] - 104s 77ms/step - loss: 0.2812 - accuracy: 0.9110 - val_loss: 0.4477 - val_accuracy: 0.8655\n",
            "Epoch 3/5\n",
            "1358/1358 [==============================] - 106s 78ms/step - loss: 0.1362 - accuracy: 0.9553 - val_loss: 0.4321 - val_accuracy: 0.8863\n",
            "Epoch 4/5\n",
            "1358/1358 [==============================] - 103s 76ms/step - loss: 0.0891 - accuracy: 0.9708 - val_loss: 0.5433 - val_accuracy: 0.8715\n",
            "Epoch 5/5\n",
            "1358/1358 [==============================] - 109s 81ms/step - loss: 0.0761 - accuracy: 0.9760 - val_loss: 0.5091 - val_accuracy: 0.8828\n"
          ]
        }
      ],
      "source": [
        "# Training the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,  # Number of steps per epoch\n",
        "    epochs=5,  # Number of epochs\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size  # Validation steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjQfNu7QwZjw"
      },
      "source": [
        "**Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9SRLiOMv3qm",
        "outputId": "c5680df7-67df-41c9-84c7-bee0b17b7f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model...\n",
            "339/339 [==============================] - 19s 57ms/step - loss: 0.5091 - accuracy: 0.8828\n",
            "Validation Accuracy: 88.28%\n"
          ]
        }
      ],
      "source": [
        "# Model Evaluation\n",
        "print(\"Evaluating model...\")\n",
        "val_loss, val_accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "ZxP07UNywYPj",
        "outputId": "228b5e7e-2a21-4cbb-931f-55708ed0cc34"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIeDSJa5xkpy"
      },
      "source": [
        "**Building a Predictive System**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0onhRrVkv9-M"
      },
      "outputs": [],
      "source": [
        "# Function to Load and Preprocess the Image using Pillow\n",
        "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
        "    # Load the image\n",
        "    img = Image.open(image_path)\n",
        "    # Resize the image\n",
        "    img = img.resize(target_size)\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = np.array(img)\n",
        "    # Add batch dimension\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    # Scale the image values to [0, 1]\n",
        "    img_array = img_array.astype('float32') / 255.\n",
        "    return img_array\n",
        "\n",
        "# Function to Predict the Class of an Image\n",
        "def predict_image_class(model, image_path, class_indices):\n",
        "    preprocessed_img = load_and_preprocess_image(image_path)\n",
        "    predictions = model.predict(preprocessed_img)\n",
        "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "    predicted_class_name = class_indices[predicted_class_index]\n",
        "    return predicted_class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YZkE2k6gwgOR"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from class indices to class names\n",
        "class_indices = {v: k for k, v in train_generator.class_indices.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dja767dwzFH",
        "outputId": "d0344da3-a583-459c-94dc-d63027b1f4a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'Apple___Apple_scab',\n",
              " 1: 'Apple___Black_rot',\n",
              " 2: 'Apple___Cedar_apple_rust',\n",
              " 3: 'Apple___healthy',\n",
              " 4: 'Blueberry___healthy',\n",
              " 5: 'Cherry_(including_sour)___Powdery_mildew',\n",
              " 6: 'Cherry_(including_sour)___healthy',\n",
              " 7: 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
              " 8: 'Corn_(maize)___Common_rust_',\n",
              " 9: 'Corn_(maize)___Northern_Leaf_Blight',\n",
              " 10: 'Corn_(maize)___healthy',\n",
              " 11: 'Grape___Black_rot',\n",
              " 12: 'Grape___Esca_(Black_Measles)',\n",
              " 13: 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
              " 14: 'Grape___healthy',\n",
              " 15: 'Orange___Haunglongbing_(Citrus_greening)',\n",
              " 16: 'Peach___Bacterial_spot',\n",
              " 17: 'Peach___healthy',\n",
              " 18: 'Pepper,_bell___Bacterial_spot',\n",
              " 19: 'Pepper,_bell___healthy',\n",
              " 20: 'Potato___Early_blight',\n",
              " 21: 'Potato___Late_blight',\n",
              " 22: 'Potato___healthy',\n",
              " 23: 'Raspberry___healthy',\n",
              " 24: 'Soybean___healthy',\n",
              " 25: 'Squash___Powdery_mildew',\n",
              " 26: 'Strawberry___Leaf_scorch',\n",
              " 27: 'Strawberry___healthy',\n",
              " 28: 'Tomato___Bacterial_spot',\n",
              " 29: 'Tomato___Early_blight',\n",
              " 30: 'Tomato___Late_blight',\n",
              " 31: 'Tomato___Leaf_Mold',\n",
              " 32: 'Tomato___Septoria_leaf_spot',\n",
              " 33: 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
              " 34: 'Tomato___Target_Spot',\n",
              " 35: 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
              " 36: 'Tomato___Tomato_mosaic_virus',\n",
              " 37: 'Tomato___healthy'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "StM3_I3UwjFV"
      },
      "outputs": [],
      "source": [
        "# saving the class names as json file\n",
        "json.dump(class_indices, open('class_indices.json', 'w'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJb9gQGRw2Ln",
        "outputId": "f329cc1c-2945-416a-f42d-174a433ff60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 266ms/step\n",
            "Predicted Class Name: Apple___Black_rot\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "image_path = '/content/test_apple_black_rot.JPG'\n",
        "#image_path = '/content/test_blueberry_healthy.jpg'\n",
        "#image_path = '/content/test_potato_early_blight.jpg'\n",
        "predicted_class_name = predict_image_class(model, image_path, class_indices)\n",
        "\n",
        "# Output the result\n",
        "print(\"Predicted Class Name:\", predicted_class_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBkknsKMyDbs"
      },
      "source": [
        "**Save the model to Google drive or local**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfoTNemcxjk5"
      },
      "outputs": [],
      "source": [
        "model.save('drive/MyDrive/trained_models/plant_disease_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ByAMH6ykbN",
        "outputId": "8836c7a9-6d35-421f-b36c-f6fb50fd5cf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('plant_disease_prediction_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install Additional Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln01Rmj0L8Hg"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless seaborn tqdm -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Robustness Testing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"✓ Robustness testing libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Corruption Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageCorruptions:\n",
        "    \"\"\"Apply realistic corruptions to test images\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_gaussian_noise(img, severity=3):\n",
        "        \"\"\"Add Gaussian noise (sensor noise)\"\"\"\n",
        "        noise_levels = [0.04, 0.08, 0.12, 0.16, 0.20]\n",
        "        std = noise_levels[severity - 1]\n",
        "        noise = np.random.normal(0, std * 255, img.shape)\n",
        "        noisy = np.clip(img + noise, 0, 255)\n",
        "        return noisy.astype(np.uint8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_salt_pepper_noise(img, severity=3):\n",
        "        \"\"\"Salt & pepper noise (transmission errors)\"\"\"\n",
        "        amounts = [0.01, 0.02, 0.03, 0.05, 0.08]\n",
        "        amount = amounts[severity - 1]\n",
        "        noisy = np.copy(img)\n",
        "        \n",
        "        # Salt\n",
        "        num_salt = int(amount * img.size * 0.5)\n",
        "        coords = [np.random.randint(0, i - 1, num_salt) for i in img.shape[:2]]\n",
        "        noisy[coords[0], coords[1], :] = 255\n",
        "        \n",
        "        # Pepper\n",
        "        num_pepper = int(amount * img.size * 0.5)\n",
        "        coords = [np.random.randint(0, i - 1, num_pepper) for i in img.shape[:2]]\n",
        "        noisy[coords[0], coords[1], :] = 0\n",
        "        \n",
        "        return noisy\n",
        "    \n",
        "    @staticmethod\n",
        "    def apply_motion_blur(img, severity=3):\n",
        "        \"\"\"Motion blur (camera shake)\"\"\"\n",
        "        kernel_sizes = [3, 5, 7, 9, 11]\n",
        "        size = kernel_sizes[severity - 1]\n",
        "        kernel = np.zeros((size, size))\n",
        "        kernel[int((size-1)/2), :] = np.ones(size)\n",
        "        kernel = kernel / size\n",
        "        blurred = cv2.filter2D(img, -1, kernel)\n",
        "        return blurred\n",
        "    \n",
        "    @staticmethod\n",
        "    def apply_gaussian_blur(img, severity=3):\n",
        "        \"\"\"Gaussian blur (out of focus)\"\"\"\n",
        "        kernel_sizes = [3, 5, 7, 9, 11]\n",
        "        ksize = kernel_sizes[severity - 1]\n",
        "        blurred = cv2.GaussianBlur(img, (ksize, ksize), 0)\n",
        "        return blurred\n",
        "    \n",
        "    @staticmethod\n",
        "    def adjust_brightness(img, severity=3):\n",
        "        \"\"\"Adjust brightness (too dark/bright)\"\"\"\n",
        "        factors = [0.4, 0.6, 0.8, 1.3, 1.6]\n",
        "        factor = factors[severity - 1]\n",
        "        adjusted = np.clip(img * factor, 0, 255)\n",
        "        return adjusted.astype(np.uint8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def adjust_contrast(img, severity=3):\n",
        "        \"\"\"Adjust contrast\"\"\"\n",
        "        factors = [0.5, 0.7, 0.9, 1.3, 1.6]\n",
        "        factor = factors[severity - 1]\n",
        "        mean = img.mean()\n",
        "        adjusted = np.clip((img - mean) * factor + mean, 0, 255)\n",
        "        return adjusted.astype(np.uint8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def apply_fog(img, severity=3):\n",
        "        \"\"\"Simulate fog\"\"\"\n",
        "        fog_levels = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "        fog_strength = fog_levels[severity - 1]\n",
        "        fog = np.ones_like(img) * 200\n",
        "        foggy = cv2.addWeighted(img, 1 - fog_strength, fog.astype(np.uint8), fog_strength, 0)\n",
        "        return foggy\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_shadow(img, severity=3):\n",
        "        \"\"\"Add shadow effect\"\"\"\n",
        "        shadow_strengths = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "        strength = shadow_strengths[severity - 1]\n",
        "        \n",
        "        h, w = img.shape[:2]\n",
        "        shadow_mask = np.zeros((h, w), dtype=np.float32)\n",
        "        \n",
        "        for i in range(h):\n",
        "            shadow_mask[i, :int(w * (1 - i/h))] = 1\n",
        "        \n",
        "        shadow_mask = cv2.GaussianBlur(shadow_mask, (51, 51), 0)\n",
        "        \n",
        "        shadowed = img.copy()\n",
        "        for c in range(3):\n",
        "            shadowed[:, :, c] = np.clip(\n",
        "                img[:, :, c] * (1 - strength * shadow_mask), 0, 255\n",
        "            )\n",
        "        \n",
        "        return shadowed.astype(np.uint8)\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_occlusion(img, severity=3):\n",
        "        \"\"\"Add random occlusions\"\"\"\n",
        "        occlusion_ratios = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "        ratio = occlusion_ratios[severity - 1]\n",
        "        \n",
        "        h, w = img.shape[:2]\n",
        "        occluded = img.copy()\n",
        "        \n",
        "        num_occlusions = np.random.randint(2, 5)\n",
        "        for _ in range(num_occlusions):\n",
        "            x1 = np.random.randint(0, w)\n",
        "            y1 = np.random.randint(0, h)\n",
        "            size = int(min(h, w) * ratio)\n",
        "            x2 = min(x1 + size, w)\n",
        "            y2 = min(y1 + size, h)\n",
        "            \n",
        "            color = np.random.randint(20, 80, 3)\n",
        "            occluded[y1:y2, x1:x2] = color\n",
        "        \n",
        "        return occluded\n",
        "\n",
        "print(\"✓ Corruption functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize Sample Corruptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_corruptions(sample_image_path, class_indices):\n",
        "    \"\"\"Visualize all corruption types on a sample image\"\"\"\n",
        "    \n",
        "    # Load sample image\n",
        "    from PIL import Image\n",
        "    img = Image.open(sample_image_path)\n",
        "    img = img.resize((224, 224))\n",
        "    img_array = np.array(img)\n",
        "    \n",
        "    corruptions = [\n",
        "        (ImageCorruptions.add_gaussian_noise, 'Gaussian Noise'),\n",
        "        (ImageCorruptions.add_salt_pepper_noise, 'Salt & Pepper'),\n",
        "        (ImageCorruptions.apply_motion_blur, 'Motion Blur'),\n",
        "        (ImageCorruptions.apply_gaussian_blur, 'Gaussian Blur'),\n",
        "        (ImageCorruptions.adjust_brightness, 'Dark'),\n",
        "        (ImageCorruptions.adjust_contrast, 'Low Contrast'),\n",
        "        (ImageCorruptions.apply_fog, 'Fog'),\n",
        "        (ImageCorruptions.add_shadow, 'Shadow'),\n",
        "        (ImageCorruptions.add_occlusion, 'Occlusion'),\n",
        "    ]\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(img_array)\n",
        "    axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Corrupted images\n",
        "    for idx, (corrupt_fn, name) in enumerate(corruptions, 1):\n",
        "        corrupted = corrupt_fn(img_array, severity=3)\n",
        "        axes[idx].imshow(corrupted)\n",
        "        axes[idx].set_title(name, fontsize=11)\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    # Hide remaining subplots\n",
        "    for idx in range(len(corruptions) + 1, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Sample Image Corruptions (Severity=3)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_corruptions.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"✓ Sample corruptions visualized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Robustness Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_robustness(model, validation_generator, class_indices, \n",
        "                        severities=[1, 3, 5], max_batches=50):\n",
        "    \"\"\"\n",
        "    Evaluate model robustness on corruptions\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        validation_generator: Keras ImageDataGenerator\n",
        "        class_indices: Dictionary mapping indices to class names\n",
        "        severities: List of severity levels to test\n",
        "        max_batches: Maximum batches to test (for speed)\n",
        "    \"\"\"\n",
        "    \n",
        "    corruptions = [\n",
        "        (ImageCorruptions.add_gaussian_noise, 'Gaussian Noise'),\n",
        "        (ImageCorruptions.add_salt_pepper_noise, 'Salt & Pepper'),\n",
        "        (ImageCorruptions.apply_motion_blur, 'Motion Blur'),\n",
        "        (ImageCorruptions.apply_gaussian_blur, 'Gaussian Blur'),\n",
        "        (ImageCorruptions.adjust_brightness, 'Brightness'),\n",
        "        (ImageCorruptions.adjust_contrast, 'Contrast'),\n",
        "        (ImageCorruptions.apply_fog, 'Fog'),\n",
        "        (ImageCorruptions.add_shadow, 'Shadow'),\n",
        "        (ImageCorruptions.add_occlusion, 'Occlusion'),\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ROBUSTNESS EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for corruption_fn, corruption_name in corruptions:\n",
        "        for severity in severities:\n",
        "            print(f\"\\nTesting: {corruption_name} (Severity {severity})\")\n",
        "            \n",
        "            y_true = []\n",
        "            y_pred_clean = []\n",
        "            y_pred_corrupt = []\n",
        "            \n",
        "            # Reset generator\n",
        "            validation_generator.reset()\n",
        "            \n",
        "            for batch_idx in tqdm(range(min(max_batches, len(validation_generator)))):\n",
        "                images, labels = next(validation_generator)\n",
        "                \n",
        "                # Clean predictions\n",
        "                preds_clean = model.predict(images, verbose=0)\n",
        "                y_pred_clean.extend(np.argmax(preds_clean, axis=1))\n",
        "                \n",
        "                # Apply corruption\n",
        "                corrupted_images = []\n",
        "                for img in images:\n",
        "                    img_uint8 = (img * 255).astype(np.uint8)\n",
        "                    corrupted = corruption_fn(img_uint8, severity=severity)\n",
        "                    corrupted_norm = corrupted.astype(np.float32) / 255.0\n",
        "                    corrupted_images.append(corrupted_norm)\n",
        "                \n",
        "                corrupted_batch = np.array(corrupted_images)\n",
        "                \n",
        "                # Corrupted predictions\n",
        "                preds_corrupt = model.predict(corrupted_batch, verbose=0)\n",
        "                y_pred_corrupt.extend(np.argmax(preds_corrupt, axis=1))\n",
        "                \n",
        "                y_true.extend(np.argmax(labels, axis=1))\n",
        "            \n",
        "            # Calculate metrics\n",
        "            acc_clean = accuracy_score(y_true, y_pred_clean)\n",
        "            acc_corrupt = accuracy_score(y_true, y_pred_corrupt)\n",
        "            degradation = acc_clean - acc_corrupt\n",
        "            \n",
        "            result = {\n",
        "                'corruption': corruption_name,\n",
        "                'severity': severity,\n",
        "                'clean_accuracy': float(acc_clean),\n",
        "                'corrupted_accuracy': float(acc_corrupt),\n",
        "                'accuracy_drop': float(degradation),\n",
        "                'relative_drop_%': float((degradation / acc_clean) * 100) if acc_clean > 0 else 0\n",
        "            }\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"  Clean: {acc_clean:.3f} | Corrupt: {acc_corrupt:.3f} | Drop: {degradation:.3f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ Robustness evaluator defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_robustness_results(results, model_name='Model'):\n",
        "    \"\"\"Create visualizations of robustness results\"\"\"\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs('robustness_results', exist_ok=True)\n",
        "    \n",
        "    # 1. Average accuracy drop by corruption type\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    avg_by_corruption = df.groupby('corruption')['accuracy_drop'].mean().sort_values()\n",
        "    avg_by_corruption.plot(kind='barh', ax=ax, color='coral')\n",
        "    ax.set_xlabel('Average Accuracy Drop', fontsize=12)\n",
        "    ax.set_title(f'{model_name} - Robustness to Corruptions', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('robustness_results/corruption_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Accuracy vs Severity\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "    for corruption in df['corruption'].unique():\n",
        "        subset = df[df['corruption'] == corruption]\n",
        "        ax.plot(subset['severity'], subset['corrupted_accuracy'], \n",
        "               marker='o', label=corruption, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Severity Level', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax.set_title(f'{model_name} - Accuracy vs Corruption Severity', fontsize=14, fontweight='bold')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('robustness_results/severity_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. Heatmap of accuracy drop\n",
        "    pivot = df.pivot(index='corruption', columns='severity', values='accuracy_drop')\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Accuracy Drop'})\n",
        "    ax.set_title(f'{model_name} - Accuracy Drop Heatmap', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Severity Level', fontsize=12)\n",
        "    ax.set_ylabel('Corruption Type', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('robustness_results/heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"✓ Visualizations saved to 'robustness_results/' folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_robustness_results(results, model_name='Model'):\n",
        "    \"\"\"Save results to CSV and JSON\"\"\"\n",
        "    \n",
        "    os.makedirs('robustness_results', exist_ok=True)\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    # Save CSV\n",
        "    csv_path = f'robustness_results/{model_name}_robustness.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"✓ CSV saved: {csv_path}\")\n",
        "    \n",
        "    # Save JSON\n",
        "    json_path = f'robustness_results/{model_name}_robustness.json'\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"✓ JSON saved: {json_path}\")\n",
        "    \n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ROBUSTNESS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nAverage accuracy drop: {df['accuracy_drop'].mean():.3f} ({df['relative_drop_%'].mean():.1f}%)\")\n",
        "    print(f\"Maximum accuracy drop: {df['accuracy_drop'].max():.3f}\")\n",
        "    print(f\"\\nMost vulnerable corruptions:\")\n",
        "    print(df.groupby('corruption')['accuracy_drop'].mean().sort_values(ascending=False).head())\n",
        "    print(f\"\\nMost robust to:\")\n",
        "    print(df.groupby('corruption')['accuracy_drop'].mean().sort_values().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RUN ROBUSTNESS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample corruptions first\n",
        "print(\"Step 1: Visualizing sample corruptions...\")\n",
        "# Pick any test image from your dataset\n",
        "sample_image = '/content/plantvillage dataset/color/Apple___Black_rot/00a914bb-b9e6-4774-ad3c-1b0a30f96b5e___JR_FrgE.S 2993.JPG'\n",
        "visualize_corruptions(sample_image, class_indices)\n",
        "\n",
        "# Run evaluation\n",
        "print(\"\\nStep 2: Running robustness evaluation...\")\n",
        "print(\"This will take 10-20 minutes depending on GPU...\")\n",
        "\n",
        "robustness_results = evaluate_robustness(\n",
        "    model=model,\n",
        "    validation_generator=validation_generator,\n",
        "    class_indices=class_indices,\n",
        "    severities=[1, 3, 5],  # Test mild, moderate, severe\n",
        "    max_batches=50  # Adjust based on time/accuracy tradeoff\n",
        ")\n",
        "\n",
        "# Visualize results\n",
        "print(\"\\nStep 3: Creating visualizations...\")\n",
        "visualize_robustness_results(robustness_results, model_name='CNN_Model')\n",
        "\n",
        "# Save results\n",
        "print(\"\\nStep 4: Saving results...\")\n",
        "save_robustness_results(robustness_results, model_name='CNN_Model')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ ROBUSTNESS EVALUATION COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nCheck the 'robustness_results' folder for:\")\n",
        "print(\"  - CSV file with detailed results\")\n",
        "print(\"  - JSON file for further analysis\")\n",
        "print(\"  - Visualization plots\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_multiple_models(model_paths, model_names, validation_generator, class_indices):\n",
        "    \"\"\"Compare robustness across multiple models\"\"\"\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for model_path, model_name in zip(model_paths, model_names):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Evaluating {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        # Load model\n",
        "        from tensorflow import keras\n",
        "        model = keras.models.load_model(model_path)\n",
        "        \n",
        "        # Evaluate\n",
        "        results = evaluate_robustness(\n",
        "            model=model,\n",
        "            validation_generator=validation_generator,\n",
        "            class_indices=class_indices,\n",
        "            severities=[3],  # Just test moderate severity for comparison\n",
        "            max_batches=50\n",
        "        )\n",
        "        \n",
        "        all_results[model_name] = results\n",
        "    \n",
        "    # Comparative visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    for model_name, results in all_results.items():\n",
        "        df = pd.DataFrame(results)\n",
        "        avg_by_corruption = df.groupby('corruption')['accuracy_drop'].mean()\n",
        "        ax.plot(avg_by_corruption.index, avg_by_corruption.values, \n",
        "               marker='o', label=model_name, linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Corruption Type', fontsize=12)\n",
        "    ax.set_ylabel('Average Accuracy Drop', fontsize=12)\n",
        "    ax.set_title('Model Comparison - Robustness to Corruptions', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('robustness_results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Model comparison complete!\")\n",
        "    \n",
        "    return all_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
